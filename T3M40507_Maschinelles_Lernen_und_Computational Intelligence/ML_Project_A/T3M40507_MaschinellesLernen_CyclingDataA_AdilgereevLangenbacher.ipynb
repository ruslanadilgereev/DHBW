{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e50299c",
   "metadata": {},
   "source": [
    "# Programmentwurf - Maschinelles Lernen\n",
    "\n",
    "Jupyter-Notebook für den Datensatz \"Cycling Data - A - bike_rides_001.csv\" in Rahmen des Moduls \"Computational Intelligence und Maschinelles Lernen\" von Ruslan Adilgereev und Valentin Langenbacher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e736c5ca",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Import Pandas: Required for data manipulation.\n",
    "Specify CSV File Path: Initialize the file path for the dataset.\n",
    "Load Data: Read the dataset into a Pandas DataFrame using a semicolon as the delimiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7262eee04b9b58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T13:57:37.078370700Z",
     "start_time": "2023-10-25T13:57:36.932186300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Pandas for data handling\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the CSV file path for the training data\n",
    "training_data_path = \"bike_rides_001.csv\"\n",
    "\n",
    "# Load the training data into a DataFrame\n",
    "df = pd.read_csv(training_data_path, sep=\";\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23a0aa4",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "- Data Cleaning: Unnecessary rows and irrelevant features are removed to simplify the dataset.\n",
    "- Time Feature Engineering: Time-related features are converted and simplified to create a new feature that represents the ride duration.\n",
    "- Data Type Standardization: Numeric data are standardized to a common format.\n",
    "- Categorical Variable Transformation: Categorical variables are transformed into a format that can be provided to machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51d0aadbd177c33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T13:57:37.167818600Z",
     "start_time": "2023-10-25T13:57:36.955763800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initial Data Preprocessing\n",
    "\n",
    "# Remove the last row, as it contains header information\n",
    "df = df.drop(df.index[len(df) - 1])\n",
    "\n",
    "# Select relevant features for the analysis\n",
    "# 'Hm', 'km', 'maxHm', 'max_speed', 'minHm', 'speed', 'type', 'ride_type', 'endtime', 'starttime'\n",
    "# are selected based on their potential relevance for the model.\n",
    "df = df[['Hm', 'km', 'maxHm', 'max_speed', 'minHm', 'speed', 'type', 'ride_type', 'endtime', 'starttime']]\n",
    "\n",
    "# Convert 'starttime' and 'endtime' to datetime format for easier manipulation\n",
    "df['starttime'] = pd.to_datetime(df['starttime'], format='%H:%M:%S')\n",
    "df['endtime'] = pd.to_datetime(df['endtime'], format='%H:%M:%S')\n",
    "\n",
    "# Calculate the ride duration in minutes and add it as a new column\n",
    "df['duration_min'] = (df['endtime'] - df['starttime']).dt.total_seconds() / 60.0\n",
    "\n",
    "# Drop the original 'starttime' and 'endtime' columns\n",
    "df = df.drop(['starttime', 'endtime'], axis=1)\n",
    "\n",
    "# Columns that need to be converted to float type\n",
    "columns_to_convert = ['Hm', 'km', 'maxHm', 'max_speed', 'minHm', 'speed']\n",
    "\n",
    "# Convert selected columns to float type\n",
    "for col in columns_to_convert:\n",
    "    df[col] = df[col].str.replace(',', '.').astype('float64')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4b280f",
   "metadata": {},
   "source": [
    "## Scaling / Normalization\n",
    "Data Normalization and Standardization: Feature scaling is essential in machine learning models that calculate distances or gradients to optimize performance and ensure that each feature contributes equally to the outcome. Two common methods are used:\n",
    "\n",
    "Min-Max Scaling: Transforms features by scaling them to a fixed range, usually [0, 1]. It is useful when the parameters have to be transformed into a bounded interval.\n",
    "\n",
    "Z-Normalization (or Standardization): Transforms features to have a mean of 0 and a standard deviation of 1, which can be crucial for algorithms that assume features are centered around zero.\n",
    "\n",
    "The Dataframe has a lot of extreme values, so the Min-Max Scaling is not the best choice. The Z-Normalization is a better choice for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1582168919a6fc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T13:57:37.181766800Z",
     "start_time": "2023-10-25T13:57:36.979201200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing the scaling functions from scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Data Scaling\n",
    "\n",
    "# Define the columns that will be scaled\n",
    "columns_features = ['Hm', 'km', 'maxHm', 'max_speed', 'minHm', 'speed', 'duration_min']\n",
    "\n",
    "# Initialize the MinMaxScaler and StandardScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "# Apply Min-Max scaling to normalize the features within the range [0, 1]\n",
    "df_minmax = pd.DataFrame(min_max_scaler.fit_transform(df[columns_features]), columns=columns_features)\n",
    "\n",
    "# Apply Z-Normalization to standardize the features with mean=0 and variance=1\n",
    "df_znorm = pd.DataFrame(standard_scaler.fit_transform(df[columns_features]), columns=columns_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87519b3a",
   "metadata": {},
   "source": [
    "Feature Engineering: After scaling the numerical features, the following step focuses on incorporating the transformed categorical variables back into the main dataset. One-hot encoded columns are extracted and then concatenated to the DataFrame to ensure that it is ready for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0d96725e8d5971",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T13:57:37.194678300Z",
     "start_time": "2023-10-25T13:57:36.994257600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform One Hot Encoding for categorical variables 'type' and 'ride_type'\n",
    "# type (MTB, Trecking, Rennrad)\n",
    "# ride_type (Pendeln, Rennen, Training)\n",
    "df = pd.get_dummies(df, columns=['type', 'ride_type'])\n",
    "\n",
    "# Extract only the one-hot encoded columns from df_onehot\n",
    "df_onehot_only = df.drop(columns=columns_features)\n",
    "\n",
    "# Concatenate the one-hot encoded columns with the Min-Max scaled DataFrame\n",
    "df = pd.concat([df_znorm, df_onehot_only.reset_index(drop=True)], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b65b80acae980",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Data Review: A quick review of the dataset is performed to ensure that it only contains the necessary, preprocessed features. This is an important step for a final sanity check before moving to model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4607004bcdb67fe7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T13:57:37.205292Z",
     "start_time": "2023-10-25T13:57:37.003903900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display the first five rows of the DataFrame\n",
    "# At this point, the DataFrame only contains the selected and scaled features\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a18e69573b3f017",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Correlation Analysis: Understanding the relationships between variables is key for feature selection and model interpretation. A correlation matrix is computed and visualized as a heatmap to inspect how each feature correlates with the target variable and other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a281fa96c09e30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T13:57:37.915372Z",
     "start_time": "2023-10-25T13:57:37.025542100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import visualization libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data Correlation Analysis\n",
    "\n",
    "# Calculate the correlation matrix for the DataFrame\n",
    "corr = df.corr()\n",
    "\n",
    "# Display correlation values with the target variable 'Hm' sorted in descending order\n",
    "print(df.corr()['Hm'].sort_values(ascending=False))\n",
    "\n",
    "# Generate a heatmap to visualize the correlation matrix\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(corr, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99369a07",
   "metadata": {},
   "source": [
    "In the following step, removing attributes contributes to a reduction in dimensionality, leading to a more straightforward and computationally efficient model, particularly beneficial for extensive datasets. Additionally, it plays a crucial role in preventing overfitting and enhancing the interpretability of the model, as fewer attributes result in a model that is simpler to comprehend and analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3191187735c33819",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T13:57:37.925103600Z",
     "start_time": "2023-10-25T13:57:37.911200800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choosing important features from the correlation matrix\n",
    "important_features = ['km', 'duration_min', 'max_speed', 'maxHm', 'ride_type_Race', 'type_Rennrad']\n",
    "df = df[['Hm'] + important_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28da73b274a081c7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Data Splitting for Model Training \n",
    "- This step is crucial for training and validating machine learning models. The dataset is divided into features (X) and the target variable (y). The train_test_split function is then used to split the data into training and validation sets. An 80/20 split ratio is commonly used for a good balance between training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a403e41d2f43780",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T13:57:38.073541500Z",
     "start_time": "2023-10-25T13:57:37.923558500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing the train_test_split function for data splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = df.drop('Hm', axis=1)\n",
    "y = df['Hm']\n",
    "# Split the data into 80% training and 20% validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68d66de93b64222",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## MLP vs Linear Regression\n",
    "\n",
    "Model Training and Evaluation: Two types of regression models, Linear Regression and MLP (MLP Regressor), are trained on the dataset. Various performance metrics like MAE (Mean Absolute Error), MSE (Mean Squared Error), RMSE (Root Mean Squared Error), and R² score are calculated to evaluate and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ea016a8ce46e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T13:57:39.190489200Z",
     "start_time": "2023-10-25T13:57:37.950610800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing relevant libraries for model training and evaluation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Model Initialization and Training\n",
    "\n",
    "# Initialize the MLP Regressor and Linear Regression models\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(200, 5, 3), max_iter=1000, activation=\"relu\", random_state=42)\n",
    "lin = LinearRegression()\n",
    "\n",
    "# Train both models using the training data\n",
    "mlp.fit(X_train, y_train)\n",
    "lin.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using both models on the validation set\n",
    "mlp_y_pred = mlp.predict(X_val)\n",
    "lin_y_pred = lin.predict(X_val)\n",
    "\n",
    "# Calculate performance metrics for both models\n",
    "metrics = {\n",
    "    'Metric': ['MAE', 'MSE', 'RMSE', 'R2'],\n",
    "    'Linear Regression': [\n",
    "        mean_absolute_error(y_val, lin_y_pred),\n",
    "        mean_squared_error(y_val, lin_y_pred),\n",
    "        np.sqrt(mean_squared_error(y_val, lin_y_pred)),\n",
    "        r2_score(y_val, lin_y_pred)\n",
    "    ],\n",
    "    'MLP': [\n",
    "        mean_absolute_error(y_val, mlp_y_pred),\n",
    "        mean_squared_error(y_val, mlp_y_pred),\n",
    "        np.sqrt(mean_squared_error(y_val, mlp_y_pred)),\n",
    "        r2_score(y_val, mlp_y_pred)\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Display the metrics in a DataFrame for easier comparison\n",
    "df_metrics = pd.DataFrame(metrics)\n",
    "print(df_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4154492",
   "metadata": {},
   "source": [
    "Comparison of various configurations:\n",
    "- Hidden Layers: Start with a simple network and gradually add complexity based on training performance and generalization.\n",
    "- The number of neurons in the hidden layers can fall between the size of the input and output layers. A common practice is to start with a number that corresponds to the average of the neurons in the input and output layers.\n",
    "- Optimal Configuration (after experiments):\n",
    "Layer 1: 200 neurons; Layer 2: 5 neurons; Layer 3: 3 neurons\n",
    "\n",
    "Result: This configuration demonstrated the best metric results during iterative experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491ef57a187a1e5f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Comparison MLP/LinReg & Evaluation:\n",
    "\n",
    "- Mean Absolute Error:\n",
    "The MLP has a lower MAE of 0.116035 compared to the Linear Regression's 0.185159, indicating better prediction accuracy.\n",
    "\n",
    "- Mean Squared Error:\n",
    "With an MSE of 0.027695, the MLP outperforms the Linear Regression model, which has an MSE of 0.084675.\n",
    "\n",
    "- Root Mean Squared Error:\n",
    "The MLP's RMSE of 0.166419 is lower than the Linear Regression's 0.290989, signifying more accurate predictions.\n",
    "\n",
    "- R-Squared:\n",
    "The MLP's R^2 value of 0.931603 is higher than the Linear Regression's 0.790884, indicating a better fit to the data.\n",
    "\n",
    "In summary, based on these metrics, the MLP demonstrates superior performance over Linear Regression for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26598da1657ddf7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Model Prediction Visualization: To provide a qualitative assessment of the model's performance, the predicted outcomes from the MLP and Linear Regression models are plotted against the actual values. Two subplots are used for clear comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36a595d44e6f6f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T13:57:39.676870600Z",
     "start_time": "2023-10-25T13:57:39.202393100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualization: Comparison of Model Predictions\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "\n",
    "# First subplot for MLP predictions\n",
    "axes[0].plot(mlp_y_pred, label='MLP Predicted', color='b')\n",
    "axes[0].plot(y_val.values, label='Actual', color='g')\n",
    "axes[0].set_title('MLP Predicted vs Actual')\n",
    "axes[0].set_xlabel('Sample Index')\n",
    "axes[0].legend()\n",
    "\n",
    "# Second subplot for Linear Regression predictions\n",
    "axes[1].plot(lin_y_pred, label='Lin Predicted', color='r')\n",
    "axes[1].plot(y_val.values, label='Actual', color='g')\n",
    "axes[1].set_title('Lin Predicted vs Actual')\n",
    "axes[1].set_xlabel('Sample Index')\n",
    "axes[1].legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a646fe7580eb7d68",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Error Visualization: This step aims to visually analyze the errors in the predictions made by the MLP and Linear Regression models. The errors are plotted across the sample index to provide insights into the reliability and stability of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7536093e6035c1ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T13:57:40.263839100Z",
     "start_time": "2023-10-25T13:57:39.680827300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualization: Model Prediction Errors\n",
    "\n",
    "# Calculate the prediction errors for both MLP and Linear Regression\n",
    "mlp_error = mlp_y_pred - y_val.values\n",
    "lin_error = lin_y_pred - y_val.values\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "\n",
    "# First subplot for MLP prediction errors\n",
    "axes[0].plot(mlp_error, label='MLP Error', color='b')\n",
    "axes[0].axhline(0, color='gray', linestyle='--')  # Add horizontal line at y=0\n",
    "axes[0].set_title('Error in MLP Predictions')\n",
    "axes[0].set_xlabel('Sample Index')\n",
    "axes[0].set_ylabel('Prediction Error')\n",
    "axes[0].legend()\n",
    "\n",
    "# Second subplot for Linear Regression prediction errors\n",
    "axes[1].plot(lin_error, label='Lin Error', color='r')\n",
    "axes[1].axhline(0, color='gray', linestyle='--')  # Add horizontal line at y=0\n",
    "axes[1].set_title('Error in Linear Regression Predictions')\n",
    "axes[1].set_xlabel('Sample Index')\n",
    "axes[1].set_ylabel('Prediction Error')\n",
    "axes[1].legend()\n",
    "\n",
    "# Add overall title and display the plot\n",
    "fig.suptitle('Prediction Errors for MLP and Linear Regression Models')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c91ca2",
   "metadata": {},
   "source": [
    "In summary, the charts depict how accurate or inaccurate each model is in predicting across various samples. The MLP model seems to generally have smaller errors, whereas the linear regression model exhibits some significant deviations, especially for certain specific samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e63771dd8104fe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Residual Analysis: Histograms of residuals (the differences between the predicted and actual values) for both MLP and Linear Regression models are plotted. This visualization helps in understanding the distribution of errors and in identifying any systematic issues in the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb11226d675c50fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T13:57:41.108168300Z",
     "start_time": "2023-10-25T13:57:40.264335300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualization: Histogram of Model Residuals\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "\n",
    "# First subplot for MLP residuals\n",
    "axes[0].hist(mlp_error, bins=50, edgecolor='black', color='b', alpha=0.7)\n",
    "axes[0].axvline(0, color='gray', linestyle='--')  # Add vertical line at x=0\n",
    "axes[0].set_title('Histogram of Residuals for MLP')\n",
    "axes[0].set_xlabel('Residual Value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Second subplot for Linear Regression residuals\n",
    "axes[1].hist(lin_error, bins=50, edgecolor='black', color='r', alpha=0.7)\n",
    "axes[1].axvline(0, color='gray', linestyle='--')  # Add vertical line at x=0\n",
    "axes[1].set_title('Histogram of Residuals for Linear Regression')\n",
    "axes[1].set_xlabel('Residual Value')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "# Add overall title and display the plot\n",
    "fig.suptitle('Histograms of Residuals for MLP and Linear Regression Models')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bb68f3d409b3bc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The boxplot of the MLP is closer to a shape of normal distribution, therefore it fits better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc1f87c90ecbd93",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Residuals vs Fitted Values: This scatter plot visually compares the residuals (errors) against the fitted values (predictions) for both MLP and Linear Regression models. It helps in identifying any non-linearity or patterns in the residuals, which could indicate model inadequacies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d559776c387ec134",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T13:57:41.617274700Z",
     "start_time": "2023-10-25T13:57:41.108661700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualization: Residuals vs Fitted Values\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "\n",
    "# First subplot for MLP residuals\n",
    "axes[0].scatter(mlp_y_pred, mlp_error, color='b', alpha=0.5)\n",
    "axes[0].axhline(y=0, color='gray', linestyle='--')  # Add horizontal line at y=0\n",
    "axes[0].set_title('Residuals vs Fitted Values for MLP')\n",
    "axes[0].set_xlabel('Fitted Values')\n",
    "axes[0].set_ylabel('Residuals')\n",
    "\n",
    "# Second subplot for Linear Regression residuals\n",
    "axes[1].scatter(lin_y_pred, lin_error, color='r', alpha=0.5)\n",
    "axes[1].axhline(y=0, color='gray', linestyle='--')  # Add horizontal line at y=0\n",
    "axes[1].set_title('Residuals vs Fitted Values for Linear Regression')\n",
    "axes[1].set_xlabel('Fitted Values')\n",
    "axes[1].set_ylabel('Residuals')\n",
    "\n",
    "# Add overall title and display the plot\n",
    "fig.suptitle('Residuals vs Fitted Values for MLP and Linear Regression Models')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8531c2e1453071f2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The distribution of the Residuals around zero is not significantly different in the plots, therefore it is hard to assume which model fits better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f91acf",
   "metadata": {},
   "source": [
    "Normal Q-Q Plots: Quantile-Quantile (Q-Q) plots are used to assess if the residuals follow a normal distribution, an important assumption in many statistical models. The Q-Q plots for both MLP and Linear Regression models are generated to compare the distribution of residuals against a standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55be50832cde25a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T13:57:42.227228Z",
     "start_time": "2023-10-25T13:57:41.619664Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualization: Normal Q-Q Plots for Model Residuals\n",
    "\n",
    "# Import statistical functions from scipy\n",
    "import scipy.stats as stats  \n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "\n",
    "# First subplot for MLP residuals\n",
    "stats.probplot(mlp_error, dist=\"norm\", plot=axes[0])\n",
    "axes[0].set_title('Normal Q-Q Plot for MLP Residuals')\n",
    "\n",
    "# Second subplot for Linear Regression residuals\n",
    "stats.probplot(lin_error, dist=\"norm\", plot=axes[1])\n",
    "axes[1].set_title('Normal Q-Q Plot for Linear Regression Residuals')\n",
    "\n",
    "# Add overall title and display the plot\n",
    "fig.suptitle('Normal Q-Q Plots for Residuals of MLP and Linear Regression Models')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b7f955cdc4aa8b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The Q-Q Plot visualize the distribution of the residuals against a standard normal distribution. The Q-Q plot is used to understand the distribution of the residuals. The Q-Q plot shows the residuals of the MLP and the linear regression model. The Q-Q plot shows that the residuals of the MLP are closer to the normal distribution than the residuals of the linear regression model.\n",
    "The Q-Q Plot of MLP shows that the residuals are closer to the normal distribution than the Q-Q Plot of the linear regression model. In the higher quantiles the Residuals of MLP are closer to the normal distribution than the residuals of the linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e97d996f6ac807b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Boxplot Analysis: Boxplots are utilized to visualize the distribution of the residuals (errors) for both MLP and Linear Regression models. This type of graph is particularly useful for identifying outliers and understanding the spread and skewness of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed58ae9c24c1b559",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T13:57:42.532495800Z",
     "start_time": "2023-10-25T13:57:42.225761900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualization: Boxplots of Model Residuals\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "\n",
    "# First subplot for MLP residuals\n",
    "axes[0].boxplot(mlp_error, vert=True, patch_artist=True, labels=['MLP'])\n",
    "axes[0].set_title('Boxplot of Residuals for MLP')\n",
    "axes[0].set_ylabel('Residual Value')\n",
    "\n",
    "# Second subplot for Linear Regression residuals\n",
    "axes[1].boxplot(lin_error, vert=True, patch_artist=True, labels=['Linear Regression'])\n",
    "axes[1].set_title('Boxplot of Residuals for Linear Regression')\n",
    "axes[1].set_ylabel('Residual Value')\n",
    "\n",
    "# Add overall title and display the plot\n",
    "fig.suptitle('Boxplots of Residuals for MLP and Linear Regression Models')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd1306a306ae0b6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Boxplot:\n",
    "Boxplots are used to visualize the distribution of residuals (errors) for both MLP and Linear Regression models. This type of graph is particularly useful for identifying outliers and understanding the spread and skewness of the data. The boxplot is a good way to visualize the distribution of the residuals. The boxplot shows the median, the interquartile range, and the outliers of the residuals. In this example the MLP has a better distribution of the residuals than the linear regression model, because the median is closer to zero and the interquartile range is smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc1fc46b00d32b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Loss Curve Analysis: The loss curve represents the model's performance metric (in this case, the cost or loss) across different iterations during the training phase. This curve aids in understanding the model's convergence behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ab843ac9837f43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T13:57:42.795500300Z",
     "start_time": "2023-10-25T13:57:42.531860200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualization: Loss Curve for MLP\n",
    "\n",
    "# Obtain loss curve data from the model\n",
    "mlp_loss_curve = mlp.loss_curve_\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mlp_loss_curve)\n",
    "plt.title(\"Loss Curve\", fontsize=14)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.grid(True)  # Add grid for better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4547f237398130e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Loss Curve:\n",
    "The Loss Curve shows the loss of the model over the training iterations. The loss is calculated as the difference between the predicted and actual values. The loss curve is used to understand the model's convergence behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ba401b2584de6f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Linear Regression Scatter Plot and Fitted Line: This visualization presents the actual data points alongside the line fitted by the Linear Regression model. The \"Fitted Line\" represents the model's predictions across the range of km values in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce802e3e66b5201",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T13:57:43.732824500Z",
     "start_time": "2023-10-25T13:57:42.798746600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualization: Scatter Plot and Fitted Line for Linear Regression Model\n",
    "\n",
    "# Obtain the slope (m) and intercept (b) from the trained linear model\n",
    "m = lin.coef_[0]\n",
    "b = lin.intercept_\n",
    "\n",
    "# Calculate the y-values based on the line equation y = mx + b\n",
    "y_line_km = m * X_val[\"km\"] + b\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(30, 20))\n",
    "\n",
    "# Create the scatter plot for actual data and the line for fitted values\n",
    "axes[0,0].scatter(X_val[\"km\"], y_val, label='Actual Data', alpha=0.7)\n",
    "axes[0,0].plot(X_val[\"km\"], y_line_km, label='Fitted Line', color='red')\n",
    "\n",
    "# Add labels and title\n",
    "axes[0,0].set_xlabel('X values (km)', fontsize=24)\n",
    "axes[0,0].set_ylabel('Y values', fontsize=24)\n",
    "axes[0,0].legend()\n",
    "\n",
    "y_line_maxHm = m * X_val[\"maxHm\"] + b\n",
    "\n",
    "# Create the scatter plot for actual data and the line for fitted values\n",
    "axes[1,0].scatter(X_val[\"maxHm\"], y_val, label='Actual Data', alpha=0.7)\n",
    "axes[1,0].plot(X_val[\"maxHm\"], y_line_maxHm, label='Fitted Line', color='red')\n",
    "\n",
    "# Add labels and title\n",
    "axes[1,0].set_xlabel('X values (maxHm)', fontsize=24)\n",
    "axes[1,0].set_ylabel('Y values', fontsize=24)\n",
    "axes[1,0].legend()\n",
    "\n",
    "y_line_max_speed = m * X_val[\"max_speed\"] + b\n",
    "\n",
    "# Create the scatter plot for actual data and the line for fitted values\n",
    "axes[0,1].scatter(X_val[\"max_speed\"], y_val, label='Actual Data', alpha=0.7)\n",
    "axes[0,1].plot(X_val[\"max_speed\"], y_line_max_speed, label='Fitted Line', color='red')\n",
    "\n",
    "# Add labels and title\n",
    "axes[0,1].set_xlabel('X values (max_speed)', fontsize=24)\n",
    "axes[0,1].set_ylabel('Y values', fontsize=24)\n",
    "axes[0,1].legend()\n",
    "\n",
    "y_line_duration_min = m * X_val[\"duration_min\"] + b\n",
    "\n",
    "# Create the scatter plot for actual data and the line for fitted values\n",
    "axes[1,1].scatter(X_val[\"duration_min\"], y_val, label='Actual Data', alpha=0.7)\n",
    "axes[1,1].plot(X_val[\"duration_min\"], y_line_duration_min, label='Fitted Line', color='red')\n",
    "\n",
    "# Add labels and title\n",
    "axes[1,1].set_xlabel('X values (duration_min)', fontsize=24)\n",
    "axes[1,1].set_ylabel('Y values', fontsize=24)\n",
    "axes[1,1].legend()\n",
    "fig.suptitle('Data Points and Fitted Line', fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcee7bc62ced137",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Plot of linear Regression Model: This visualization presents the actual data points alongside the line fitted by the Linear Regression model. The \"Fitted Line\" represents the model's predictions across the range of different feature values in the validation set.\n",
    "Due to multiple features, the plot is not quite useful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
